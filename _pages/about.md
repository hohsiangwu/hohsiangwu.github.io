---
permalink: /
title: "About Me"
<!--- excerpt: "About me" --->
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Ho-Hsiang Wu is currently a research scientist at [Bosch Research](https://www.bosch.com/research/). He finished his PhD in [Music Technology](https://steinhardt.nyu.edu/programs/music-technology) program at New York University, advised by [Dr. Juan Pablo Bello](https://wp.nyu.edu/jpbello/). His research interests focus on design and evaluation of audio representation learning for machine listening systems, applying self-supervised and unsupervised learning techniques to tackle limited labeled data problems.

He was a research intern at [Adobe Research](https://research.adobe.com/) and [Descript](https://www.descript.com/), and he also worked as an applied scientist intern at [Amazon Alexa](https://developer.amazon.com/en-US/alexa). Before starting his PhD journey, he worked in industry for several years building machine learning products. He holds MS degree in Electrical Engineering from [University of California, Los Angeles](https://www.ucla.edu/) and BS degree from [National Taiwan University](https://www.ntu.edu.tw/english/).

# Publications
Tatiya, G., Francis, J., **Wu, H. H.**, Bisk, Y., & Sinapov, J. (2024). MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Perception. ICRA, 2024.
<a href="https://arxiv.org/pdf/2309.08508.pdf" target="_blank"><i class="fas fa-file-pdf"></i></a>

Kim, G., **Wu, H. H.**, Bondi, L., & Liu, B. (2024). Multi-Modal Continual Pre-Training for Audio Encoders. ICASSP, 2024.
<a href="https://www.cs.uic.edu/~liub/publications/icassp_2024_gyuhak.pdf" target="_blank"><i class="fas fa-file-pdf"></i></a>

Lin, W. C., Ghaffarzadegan, S., Bondi, L., Kumar, A., Das, S., & **Wu, H. H.** (2024). CLAP4EMO: ChatGPT-Assisted Speech Emotion Retrieval with Natural Language Supervision. ICASSP, 2024.

Vosoughi, A., Bondi, L., **Wu, H. H.**, & Xu, C. (2024). Learning Audio Concepts from Counterfactual Natural Language. ICASSP, 2024.
<a href="https://arxiv.org/pdf/2401.04935.pdf" target="_blank"><i class="fas fa-file-pdf"></i></a>

Ghaffarzadegan, S., Bondi, L., **Wu, H.-H.**, Munir, S., Shields, K.J., Das, S., Aracri, J. (2023). Active Learning for Abnormal Lung Sound Data Curation and Detection in Asthma. INTERSPEECH, 2023.
<a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/ghaffarzadegan23_interspeech.pdf" target="_blank"><i class="fas fa-file-pdf"></i></a>

**Wu, H. H.**, Nieto, O., Bello, J. P., & Salomon, J. (2023). Audio-Text Models Do Not Yet Leverage Natural Language. ICASSP, 2023.
<a href="https://arxiv.org/pdf/2303.10667.pdf" target="_blank"><i class="fas fa-file-pdf"></i></a>

**Wu, H. H.**, Fuentes, M., Seetharaman, P., & Bello, J. P. (2022). How to Listen? Rethinking Visual Sound Localization. INTERSPEECH, 2022.
<a href="https://arxiv.org/pdf/2204.05156.pdf" target="_blank"><i class="fas fa-file-pdf"></i></a> <a href="https://github.com/hohsiangwu/rethinking-visual-sound-localization" target="_blank"><i class="fab fa-github"></i></a>

Srivastava, S., **Wu, H. H.**, Rulff, J., Fuentes, M., Cartwright, M., Silva, C., Arora, A. & Bello, J. P. (2022). A Study on Robustness to Perturbations for Representations of Environmental Sound. EUSIPCO, 2022.
<a href="https://arxiv.org/pdf/2203.10425.pdf" target="_blank"><i class="fas fa-file-pdf"></i></a>

**Wu, H. H.**, Seetharaman, P., Kumar, K., & Bello, J. P. (2022). Wav2CLIP: Learning Robust Audio Representations From CLIP. ICASSP, 2022.
<a href="https://arxiv.org/pdf/2110.11499.pdf" target="_blank"><i class="fas fa-file-pdf"></i></a> <a href="https://github.com/descriptinc/lyrebird-wav2clip" target="_blank"><i class="fab fa-github"></i></a>

**Wu, H. H.**, Fuentes, M., Bello, J. P. (2021). Exploring Modality-Agnostic Representations for Music Classification. SMC, 2021.
<a href="https://arxiv.org/pdf/2106.01149.pdf" target="_blank"><i class="fas fa-file-pdf"></i></a> <a href="https://github.com/hohsiangwu/crossmodal" target="_blank"><i class="fab fa-github"></i></a>

**Wu, H. H.**, Kao, C., Tang, Q., Sun, M., McFee, B., Bello, J. P., & Wang, C. (2021). Multi-task Self-supervised Pre-training for Music Classification. ICASSP, 2021.
<a href="https://arxiv.org/pdf/2102.03229.pdf" target="_blank"><i class="fas fa-file-pdf"></i></a>

Cartwright, M., Cramer, J., Mendez, A. E. M., Wang, Y., **Wu, H. H.**, Lostanlen, V., & Nov, O. (2020). SONYC-UST-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context. DCASE2020.
<a href="https://arxiv.org/pdf/2009.05188.pdf" target="_blank"><i class="fas fa-file-pdf"></i></a>

Cartwright, M., Mendez, A. Cramer, J., Lostanlen, V., Dove, G., **Wu, H.**,  Salamon, J., Nov, O., and Bello, J. P. (2019). SONYC Urban Sound Tagging (SONYC-UST): A Multilabel Dataset from an Urban Acoustic Sensor Network. DCASE2019.
<a href="https://archive.nyu.edu/bitstream/2451/60776/1/DCASE2019Workshop_Cartwright_4.pdf" target="_blank"><i class="fas fa-file-pdf"></i></a>

Husain, H., **Wu, H. H.**, Gazit, T., Allamanis, M., & Brockschmidt, M. (2019). Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436.
<a href="https://arxiv.org/pdf/1909.09436.pdf" target="_blank"><i class="fas fa-file-pdf"></i></a> <a href="https://github.com/github/CodeSearchNet" target="_blank"><i class="fab fa-github"></i></a>

Cramer, J.<sup>*</sup>, **Wu, H. H.**<sup>*</sup>, Salamon, J., & Bello, J. P. (2019). Look, Listen, and Learn More: Design Choices for Deep Audio Embeddings. ICASSP, 2019.
<a href="https://www.justinsalamon.com/uploads/4/3/9/4/4394963/cramer_looklistenlearnmore_icassp_2019.pdf" target="_blank"><i class="fas fa-file-pdf"></i></a> <a href="https://github.com/marl/openl3" target="_blank"><i class="fab fa-github"></i></a>

**Wu, H. H.**, & Bello, J. P. (2010). Audio-based music visualization for music structure analysis. In Proceedings of Sound and Music Computing Conference (SMC) (pp. 1-6).
<a href="https://core.ac.uk/download/pdf/144846461.pdf" target="_blank"><i class="fas fa-file-pdf"></i></a>

Chen, C. W., Lee, K., & **Wu, H. H.** (2009). Towards a class-based representation of perceptual tempo for music retrieval. In 2009 International Conference on Machine Learning and Applications (pp. 602-607). IEEE.
<a href="http://www.cwlabs.com/publications/ICMLA09-ChenEtAl-ClassBasedPerceptualTempo.pdf" target="_blank"><i class="fas fa-file-pdf"></i></a>

Chen, C. W., Cremer, M., Lee, K., DiMaria, P., & **Wu, H. H.** (2009). Improving perceived tempo estimation by statistical modeling of higher-level musical descriptors. In Audio Engineering Society Convention 126. Audio Engineering Society.
<a href="http://cwlabs.com/publications/AES126-ChenEtAl-ImprovingPerceivedTempoEstimation.pdf" target="_blank"><i class="fas fa-file-pdf"></i></a>

# Tutorials
Husain, H., **Wu, H. H.** (2018). Feature Extraction and Summarization with Sequence to Sequence Learning. KDD 2018 Hands-On Tutorials.
<a href="https://www.kdd.org/kdd2018/hands-on-tutorials/view/feature-extraction-and-summarization-with-sequence-to-sequence-learning" target="_blank"><i class="fas fa-link"></i></a> <a href="https://github.com/hohsiangwu/kdd-2018-hands-on-tutorials" target="_blank"><i class="fab fa-github"></i></a>
