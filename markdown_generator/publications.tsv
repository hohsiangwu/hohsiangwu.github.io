pub_date	title	venue	excerpt	citation	url_slug	paper_url
2009-05-01	Improving perceived tempo estimation by statistical modeling of higher-level musical descriptors	Audio Engineering Society Convention 126	Conventional tempo estimation algorithms generally work by detecting significant audio events and finding periodicities of repetitive patterns in an audio signal. However, human perception of tempo is subjective, and relies on a far richer set of information, causing many tempo estimation algorithms to suffer from octave errors, or “double/half-time” confusion. In this paper, we propose a system that uses higher-level musical descriptors such as mood to train a statistical model of perceived tempo classes, which can then used to correct the estimate from a conventional tempo estimation algorithm. Our experimental results show reliable classification of perceived tempo class, as well as a significant reduction of octave errors when applied to an array of available tempo estimation algorithms.	Chen, C. W., Cremer, M., Lee, K., DiMaria, P., & Wu, H. H. (2009, May). Improving perceived tempo estimation by statistical modeling of higher-level musical descriptors. In Audio Engineering Society Convention 126. Audio Engineering Society.	aes-2009	http://cwlabs.com/publications/AES126-ChenEtAl-ImprovingPerceivedTempoEstimation.pdf
2009-12-13	Towards a class-based representation of perceptual tempo for music retrieval	2009 International Conference on Machine Learning and Applications	Tempo is a common criterion by which humans describe and categorize music, and this has spawned a large amount of research in the field of automatic tempo estimation. Most tempo estimation systems focus mainly on detecting the temporal repetition and periodicity present within a signal, and represent tempo as a count of beats-per-minute (BPM). However, in real-world music retrieval applications such as music navigation and playlist generation, a rough perceptual representation of tempo may be more appropriate than a BPM representation. In this paper, the problem of tempo estimation is presented as a statistical classification problem. Four Perceptual Tempo Classes are defined which correspond to rough semantic terms that average users may use to describe tempo. Statistical models of each class are built using low-level audio features. Experimental results show that the Perceptual Tempo Class representation outperforms several conventional BPM-based tempo estimation systems when applied to the tasks of music navigation and playlist generation.	Chen, C. W., Lee, K., & Wu, H. H. (2009, December). Towards a class-based representation of perceptual tempo for music retrieval. In 2009 International Conference on Machine Learning and Applications (pp. 602-607). IEEE.	icmla-2009	http://www.cwlabs.com/publications/ICMLA09-ChenEtAl-ClassBasedPerceptualTempo.pdf
2010-07-18	Audio-based music visualization for music structure analysis	2010 Proceedings of Sound and Music Computing Conference (SMC)	We propose an approach to audio-based data-driven music visualization and an experimental design to study if the music visualization can aid listeners in identifying the structure of music. A three stage system is presented including feature extraction, the generation of a recurrence plot and the creation of an arc diagram to visualize the repetitions within a piece. Then subjects are asked to categorize simple forms of classical music with and without audio and visual cues provided. The accuracy and speed are measured. The results show that the visualization can reinforce the identification of musical forms.	Wu, H. H., & Bello, J. P. (2010). Audio-based music visualization for music structure analysis. In Proceedings of Sound and Music Computing Conference (SMC) (pp. 1-6).	smc-2010	https://core.ac.uk/download/pdf/144846461.pdf
2019-05-12	Look, listen, and learn more: Design choices for deep audio embeddings	ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	A considerable challenge in applying deep learning to audio classification is the scarcity of labeled data. An increasingly popular solution is to learn deep audio embeddings from large audio collections and use them to train shallow classifiers using small labeled datasets. Look, Listen, and Learn (L 3 -Net) is an embedding trained through self-supervised learning of audio-visual correspondence in videos as opposed to other embeddings requiring labeled data. This framework has the potential to produce powerful out-of-the-box embeddings for downstream audio classification tasks, but has a number of unexplained design choices that may impact the embeddings' behavior. In this paper we investigate how L 3 -Net design choices impact the performance of downstream audio classifiers trained with these embeddings. We show that audio-informed choices of input representation are important, and that using sufficient data for training the embedding is key. Surprisingly, we find that matching the content for training the embedding to the downstream task is not beneficial. Finally, we show that our best variant of the L 3 -Net embedding outperforms both the VGGish and SoundNet embeddings, while having fewer parameters and being trained on less data. Our implementation of the L 3 -Net embedding model as well as pre-trained models are made freely available online.	Cramer, J., Wu, H. H., Salamon, J., & Bello, J. P. (2019, May). Look, listen, and learn more: Design choices for deep audio embeddings. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 3852-3856). IEEE.	icassp-2019	https://www.justinsalamon.com/uploads/4/3/9/4/4394963/cramer_looklistenlearnmore_icassp_2019.pdf
2019-09-20	Codesearchnet challenge: Evaluating the state of semantic code search	arXiv preprint arXiv:1909.09436	Semantic code search is the task of retrieving relevant code given a natural language query. While related to other information retrieval tasks, it requires bridging the gap between the language used in code (often abbreviated and highly technical) and natural language more suitable to describe vague concepts and ideas. To enable evaluation of progress on code search, we are releasing the CodeSearchNet Corpus and are presenting the CodeSearchNet Challenge, which consists of 99 natural language queries with about 4k expert relevance annotations of likely results from CodeSearchNet Corpus. The corpus contains about 6 million functions from open-source code spanning six programming languages (Go, Java, JavaScript, PHP, Python, and Ruby). The CodeSearchNet Corpus also contains automatically generated query-like natural language for 2 million functions, obtained from mechanically scraping and preprocessing associated function documentation. In this article, we describe the methodology used to obtain the corpus and expert labels, as well as a number of simple baseline solutions for the task.	Husain, H., Wu, H. H., Gazit, T., Allamanis, M., & Brockschmidt, M. (2019). Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436.	arxiv-2019	https://arxiv.org/pdf/1909.09436.pdf
2019-10-25	Sonyc urban sound tagging (sonyc-ust): A multilabel dataset from an urban acoustic sensor network	Detection and Classification of Acoustic Scenes and Events 2019	SONYC Urban Sound Tagging (SONYC-UST) is a dataset for the development and evaluation of machine listening systems for real-world urban noise monitoring. It consists of 3068 audio recordings from the "Sounds of New York City" (SONYC) acoustic sensor network. Via the Zooniverse citizen science platform, volunteers tagged the presence of 23 fine-grained classes that were chosen in consultation with the New York City Department of Environmental Protection. These 23 fine-grained classes can be grouped into eight coarse-grained classes. In this work, we describe the collection of this dataset, metrics used to evaluate tagging systems, and the results of a simple baseline model.	M. Cartwright, A. Mendez, J. Cramer, V. Lostanlen, G. Dove, H. Wu, J. Salamon, O. Nov & J. Bello, "SONYC Urban Sound Tagging (SONYC-UST): A Multilabel Dataset from an Urban Acoustic Sensor Network", Proceedings of the Detection and Classification of Acoustic Scenes and Events 2019 Workshop (DCASE2019), pages 35–39, New York University, NY, USA, Oct. 2019	dcase-2019	https://archive.nyu.edu/bitstream/2451/60776/1/DCASE2019Workshop_Cartwright_4.pdf
2020-09-11	SONYC-UST-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context	Detection and Classification of Acoustic Scenes and Events 2020	We present SONYC-UST-V2, a dataset for urban sound tagging with spatiotemporal information. This dataset is aimed for the development and evaluation of machine listening systems for real-world urban noise monitoring. While datasets of urban recordings are available, this dataset provides the opportunity to investigate how spatiotemporal metadata can aid in the prediction of urban sound tags. SONYC-UST-V2 consists of 18510 audio recordings from the" Sounds of New York City"(SONYC) acoustic sensor network, including the timestamp of audio acquisition and location of the sensor. The dataset contains annotations by volunteers from the Zooniverse citizen science platform, as well as a two-stage verification with our team. In this article, we describe our data collection procedure and propose evaluation metrics for multilabel classification of urban sound tags. We report the results of a simple baseline model that exploits spatiotemporal information.	Cartwright, M., Cramer, J., Mendez, A. E. M., Wang, Y., Wu, H. H., Lostanlen, V., ... & Bello, J. P. (2020). SONYC-UST-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context. Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020)	dcase-2020	https://arxiv.org/pdf/2009.05188.pdf
2021-06-06	Multi-Task Self-Supervised Pre-Training for Music Classification	ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	Deep learning is very data hungry, and supervised learning especially requires massive labeled data to work well. Machine listening research often suffers from limited labeled data problem, as human annotations are costly to acquire, and annotations for audio are time consuming and less intuitive. Besides, models learned from labeled dataset often embed biases specific to that particular dataset. Therefore, unsupervised learning techniques become popular approaches in solving machine listening problems. Particularly, a self-supervised learning technique utilizing reconstructions of multiple hand-crafted audio features has shown promising results when it is applied to speech domain such as emotion recognition and automatic speech recognition (ASR). In this paper, we apply self-supervised and multi-task learning methods for pre-training music encoders, and explore various design choices including encoder architectures, weighting mechanisms to combine losses from multiple tasks, and worker selections of pretext tasks. We investigate how these design choices interact with various downstream music classification tasks. We find that using various music specific workers altogether with weighting mechanisms to balance the losses during pre-training helps improve and generalize to the downstream tasks.	Wu, H. H., Kao, C. C., Tang, Q., Sun, M., McFee, B., Bello, J. P., & Wang, C. (2021, June). Multi-Task Self-Supervised Pre-Training for Music Classification. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 556-560). IEEE.	icassp-2021	https://arxiv.org/pdf/2102.03229.pdf